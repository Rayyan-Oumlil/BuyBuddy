# ğŸ›’ BuyBuddy - Intelligent Shopping Agent

An intelligent product search service that allows users to specify what they want to buy and finds corresponding products on the internet.

BuyBuddy leverages AI agents to understand user queries, research products, compare prices, and provide intelligent shopping recommendations.

## âœ¨ Features

- ğŸ¤– **AI-Powered Query Understanding**: Natural language processing to understand user shopping intent
- ğŸ” **Intelligent Product Research**: Multi-source product search with price comparison
- ğŸ’¬ **Conversational Interface**: Chat-based interaction for seamless shopping assistance
- ğŸ”„ **Multi-LLM Support**: Flexible LLM provider system (Ollama, DeepSeek, OpenAI)
- ğŸ¯ **Workflow-Based Architecture**: LangGraph-powered state management for complex shopping workflows

## ğŸš€ Tech Stack

- **Backend**: FastAPI (Python)
- **Frontend**: React + Vite + TailwindCSS
- **API de recherche**: SerperDev (products with prices)
- **LLM**: Multi-provider support (Ollama, DeepSeek, OpenAI) - easily swappable
- **Agents**: Query Understanding + Product Researcher + Price Comparator + Conversation Handler
- **Workflow Engine**: LangGraph for state management

## ğŸ“‹ Project Status

- âœ… Backend FastAPI functional
- âœ… SerperDev API configured and tested
- âœ… Query Understanding Agent (understands user queries)
- âœ… Product Researcher Agent (intelligent product search)
- âœ… Price Comparator Agent
- âœ… Conversation Handler Agent
- âœ… Frontend React interface
- âœ… LangGraph workflow implementation

## ğŸƒ Quick Start

### Prerequisites

- Python 3.8+
- Node.js 18+
- An API key from [SerperDev](https://serper.dev/)
- (Optional) An LLM provider API key (Ollama, DeepSeek, or OpenAI)

### Backend Setup

1. Navigate to the backend directory:
```bash
cd backend
```

2. Create and activate a virtual environment:
```bash
# Windows
python -m venv venv
.\venv\Scripts\Activate.ps1

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file in the `backend` directory:
```env
# Search API (Required)
SERPER_API_KEY=your_serper_api_key

# LLM Provider (choose one)
LLM_PROVIDER=ollama  # Options: ollama, deepseek, openai

# Ollama (if LLM_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# DeepSeek (if LLM_PROVIDER=deepseek)
DEEPSEEK_API_KEY=your_deepseek_api_key

# OpenAI (if LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key
```

5. Start the backend server:
```bash
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### Frontend Setup

1. Navigate to the frontend directory:
```bash
cd frontend
```

2. Install dependencies:
```bash
npm install
```

3. Start the development server:
```bash
npm run dev
```

The frontend will be available at `http://localhost:5173`

## ğŸ“š API Documentation

Once the backend is running, visit:
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

### Main Endpoints

- `POST /api/v1/chat` - Chat with the shopping agent
- `POST /api/v1/search` - Direct product search
- `GET /api/v1/health` - Health check

## ğŸ§ª Testing

### Test Product Search
```powershell
# Windows PowerShell
Invoke-RestMethod -Uri "http://localhost:8000/api/v1/search" -Method POST -Headers @{"Content-Type"="application/json"} -Body '{"query":"laptop gaming"}'
```

```bash
# macOS/Linux
curl -X POST "http://localhost:8000/api/v1/search" \
  -H "Content-Type: application/json" \
  -d '{"query":"laptop gaming"}'
```

### Test Chat Interface
```powershell
# Windows PowerShell
Invoke-RestMethod -Uri "http://localhost:8000/api/v1/chat" -Method POST -Headers @{"Content-Type"="application/json"} -Body '{"message":"I want a gaming laptop under $1500"}'
```

## ğŸ—ï¸ Architecture

The project follows a clean architecture pattern:

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/          # AI agents (query understanding, product research, etc.)
â”‚   â”œâ”€â”€ api/             # FastAPI routes
â”‚   â”œâ”€â”€ core/            # Configuration and database
â”‚   â”œâ”€â”€ infrastructure/  # External APIs and LLM providers
â”‚   â”œâ”€â”€ models/          # Data models and schemas
â”‚   â””â”€â”€ workflows/       # LangGraph workflows
â””â”€â”€ main.py              # Application entry point

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”œâ”€â”€ hooks/           # Custom React hooks
â”‚   â””â”€â”€ App.jsx          # Main application component
```

## ğŸ”§ Configuration

**Switching LLM Providers**: Simply change the `LLM_PROVIDER` value in your `.env` file. No code changes required!

Supported providers:
- **Ollama**: Local LLM deployment (free, requires local setup)
- **DeepSeek**: Cloud-based LLM (cost-effective)
- **OpenAI**: GPT models (premium quality)

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

